{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a50655-e1a5-4cbb-8340-32d81faeaba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training loss data\n",
    "steps = list(range(10, 910, 10))\n",
    "training_loss = [\n",
    "    3.044700, 2.342800, 1.621200, 1.077000, 0.886600, 0.933800, 0.791700, 0.788900, \n",
    "    0.733100, 0.636700, 0.630700, 0.592000, 0.534600, 0.473800, 0.440700, 0.352300, \n",
    "    0.375200, 0.382200, 0.297800, 0.256300, 0.307300, 0.228200, 0.208500, 0.222500, \n",
    "    0.172700, 0.185500, 0.185400, 0.150800, 0.139100, 0.141300, 0.127800, 0.127000, \n",
    "    0.135500, 0.111500, 0.109500, 0.117600, 0.094500, 0.116900, 0.098700, 0.088300, \n",
    "    0.087600, 0.095500, 0.088800, 0.090600, 0.086500, 0.071600, 0.085900, 0.076200, \n",
    "    0.067500, 0.068800, 0.072600, 0.056600, 0.066900, 0.068000, 0.059600, 0.061800, \n",
    "    0.064800, 0.052400, 0.055500, 0.061500, 0.046700, 0.053800, 0.054600, 0.044900, \n",
    "    0.050100, 0.055600, 0.045400, 0.047400, 0.053100, 0.041100, 0.046100, 0.052400, \n",
    "    0.040400, 0.047800, 0.047600, 0.040200, 0.045900, 0.046700, 0.040700, 0.045000, \n",
    "    0.045600, 0.038100, 0.044900, 0.046000, 0.039600, 0.047100, 0.041900, 0.039300, \n",
    "    0.043200, 0.045400\n",
    "]\n",
    "\n",
    "# Plotting the training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps, training_loss, label='Training Loss', color='blue')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss of fine-tuning HuggingFaceH4/zephyr-7b-beta over Steps(Epoch=30)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed4f39a-f065-473c-a203-3c43b39a671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New training loss data\n",
    "steps_new = list(range(10, 910, 10))\n",
    "training_loss_new = [\n",
    "    3.966900, 4.030500, 3.014200, 2.244700, 2.045700, 1.753800, 1.426900, 1.467500, \n",
    "    1.382800, 1.171400, 1.250200, 1.204300, 1.008600, 1.054200, 0.991900, 0.801300, \n",
    "    0.862800, 0.830200, 0.651500, 0.639400, 0.622200, 0.487600, 0.487800, 0.502300, \n",
    "    0.382800, 0.358600, 0.399900, 0.296900, 0.291100, 0.317000, 0.236900, 0.255600, \n",
    "    0.235000, 0.193800, 0.191600, 0.215200, 0.155200, 0.169100, 0.187800, 0.132200, \n",
    "    0.156400, 0.164500, 0.121900, 0.149900, 0.141700, 0.111900, 0.140000, 0.133400, \n",
    "    0.112000, 0.127900, 0.119300, 0.101600, 0.114400, 0.120500, 0.086700, 0.113500, \n",
    "    0.117300, 0.090300, 0.099200, 0.113200, 0.080800, 0.099400, 0.113000, 0.081800, \n",
    "    0.101300, 0.098300, 0.082300, 0.094900, 0.099000, 0.083400, 0.091400, 0.095100, \n",
    "    0.075700, 0.092800, 0.094600, 0.074700, 0.086000, 0.098000, 0.073300, 0.086700, \n",
    "    0.096600, 0.080500, 0.088300, 0.085100, 0.065600, 0.088100, 0.098800, 0.077200, \n",
    "    0.088800, 0.085600\n",
    "]\n",
    "\n",
    "# Plotting the new training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps_new, training_loss_new, label='Training Loss', color='green')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss of fine-tuning meta-llama/Llama-2-7b-chat-hf over Steps(Epoch=30)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b139a8d-2df7-486f-a3d4-49dc91d848b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "steps_another = list(range(10, 910, 10))\n",
    "training_loss_another = [\n",
    "    10.797600, 10.321300, 8.261800, 4.302900, 1.834600, 1.365500, 1.073500, 1.121400,\n",
    "    1.067000, 0.916400, 0.903400, 0.827200, 0.777300, 0.705000, 0.642900, 0.529500,\n",
    "    0.531300, 0.540700, 0.416000, 0.360800, 0.403100, 0.305100, 0.292000, 0.304500,\n",
    "    0.239400, 0.236600, 0.245400, 0.196500, 0.195900, 0.210400, 0.170300, 0.174800,\n",
    "    0.185700, 0.153400, 0.156400, 0.157900, 0.124800, 0.138600, 0.144700, 0.114500,\n",
    "    0.136600, 0.126200, 0.109000, 0.118900, 0.114700, 0.098200, 0.108300, 0.097700,\n",
    "    0.091300, 0.090500, 0.093400, 0.072800, 0.081400, 0.082400, 0.069500, 0.081200,\n",
    "    0.082400, 0.064500, 0.070700, 0.075700, 0.060500, 0.069100, 0.067100, 0.053600,\n",
    "    0.062300, 0.067600, 0.053900, 0.058400, 0.066000, 0.050500, 0.056400, 0.063100,\n",
    "    0.048700, 0.056600, 0.057700, 0.047400, 0.056000, 0.056200, 0.047700, 0.054000,\n",
    "    0.054600, 0.045100, 0.053500, 0.054900, 0.047000, 0.056500, 0.049500, 0.046100,\n",
    "    0.051300, 0.053800\n",
    "]\n",
    "\n",
    "# Plotting the another training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps_another, training_loss_another, label='Training Loss', color='red')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss of fine-tuning google/gemma-1.1-7b-it over Steps')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9199e93-c89a-4ecb-9038-248b3d02fd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Latest training loss data\n",
    "steps_latest = list(range(10, 910, 10))\n",
    "training_loss_latest = [\n",
    "    4.745900, 4.172500, 2.843600, 1.994600, 1.626400, 1.438500, 1.320700, 1.342600, \n",
    "    1.284200, 1.156500, 1.181800, 1.172800, 1.056500, 1.068000, 1.007700, 0.890000, \n",
    "    0.926000, 0.891400, 0.753900, 0.767800, 0.729300, 0.614100, 0.593400, 0.591200, \n",
    "    0.491400, 0.467400, 0.481400, 0.418700, 0.395800, 0.406200, 0.355000, 0.331300, \n",
    "    0.339800, 0.311300, 0.279500, 0.291400, 0.245800, 0.242900, 0.267100, 0.205900, \n",
    "    0.219700, 0.227800, 0.183100, 0.200800, 0.195400, 0.162700, 0.156000, 0.161300, \n",
    "    0.140400, 0.135900, 0.133500, 0.119200, 0.119700, 0.116100, 0.101900, 0.105300, \n",
    "    0.112600, 0.092400, 0.098500, 0.110100, 0.082000, 0.097500, 0.098100, 0.079100, \n",
    "    0.090600, 0.093300, 0.079500, 0.083600, 0.086600, 0.073000, 0.079100, 0.084900, \n",
    "    0.068100, 0.078300, 0.080300, 0.065500, 0.073200, 0.079600, 0.063300, 0.072400, \n",
    "    0.078200, 0.067100, 0.071500, 0.070500, 0.060700, 0.071000, 0.076600, 0.064200, \n",
    "    0.072600, 0.068700\n",
    "]\n",
    "\n",
    "# Plotting the latest training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps_latest, training_loss_latest, label='Training Loss', color='purple')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss of fine-tuning meta-llama/Meta-Llama-3-8B-Instruct over Steps')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd346e2-4c96-4824-a566-58e879768bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "\n",
    "# 数据标签\n",
    "labels = ['Code', 'Commonsense Reasoning', 'World Knowledge', 'Reading Comprehension', 'Math', 'MMLU', 'BBH', 'AGI Eval']\n",
    "num_vars = len(labels)\n",
    "\n",
    "# 各个模型的数据\n",
    "data = {\n",
    "    'MPT 7B': [20.5, 57.4, 41.0, 57.5, 4.9, 26.8, 31.0, 23.5],\n",
    "    'Falcon 7B': [5.6, 56.1, 42.8, 36.0, 4.6, 26.2, 28.0, 21.2],\n",
    "    'LLaMA 1 7B': [14.1, 60.8, 46.2, 58.5, 6.95, 35.1, 30.3, 23.9],\n",
    "    'LLaMA 2 7B': [16.8, 63.9, 48.9, 61.3, 14.6, 45.3, 32.6, 29.3]\n",
    "}\n",
    "\n",
    "# 计算雷达图的角度\n",
    "angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# 初始化图形\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "\n",
    "# 绘制每个模型的数据\n",
    "for model in data:\n",
    "    values = data[model]\n",
    "    values += values[:1]  # 关闭雷达图\n",
    "    ax.plot(angles, values, linewidth=2, linestyle='solid', label=model)\n",
    "    ax.fill(angles, values, alpha=0.25)\n",
    "\n",
    "# 添加标签\n",
    "plt.xticks(angles[:-1], labels)\n",
    "\n",
    "# 添加图例\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.1, 1.1))\n",
    "\n",
    "# 显示图形\n",
    "plt.title('Performance Comparison of 7B Models')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaa0b34-844d-412b-8d92-6b5b92009256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "\n",
    "# Data labels\n",
    "labels = ['MMLU', 'GPQA', 'HumanEval', 'GSM-8K', 'MATH']\n",
    "num_vars = len(labels)\n",
    "\n",
    "# Data for each model\n",
    "data = {\n",
    "    'Llama 3 8B': [68.4, 34.2, 62.2, 79.6, 30.0],\n",
    "    'Llama 2 7B': [34.1, 21.7, 7.9, 25.7, 3.8],\n",
    "    'Llama 2 13B': [47.8, 22.3, 14.0, 77.4, 6.7],\n",
    "    'Llama 3 70B': [82.0, 39.5, 81.7, 93.0, 50.4],\n",
    "    'Llama 2 70B': [52.9, 21.0, 25.6, 57.5, 11.6]\n",
    "}\n",
    "\n",
    "# Calculate angles for radar chart\n",
    "angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Initialize radar chart\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "\n",
    "# Plot data for each model\n",
    "for model in data:\n",
    "    values = data[model]\n",
    "    values += values[:1]  # Close the radar chart\n",
    "    ax.plot(angles, values, linewidth=2, linestyle='solid', label=model)\n",
    "    ax.fill(angles, values, alpha=0.25)\n",
    "\n",
    "# Add labels\n",
    "plt.xticks(angles[:-1], labels)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.1, 1.1))\n",
    "\n",
    "# Show title\n",
    "plt.title('Performance Comparison of LLaMA Models')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7493c171-0712-41dd-b611-320ddbc69fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "path = \"\"\n",
    "dataset_llama_after_prediction = datasets.load_from_disk(path)\n",
    "print(dataset_llama_after_prediction)\n",
    "\n",
    "print(dataset_llama_after_prediction[1]['input_for_zephyr'])\n",
    "print('=====================================================')\n",
    "print(dataset_llama_after_prediction[1]['completion'])\n",
    "print('=====================================================')\n",
    "print(dataset_llama_after_prediction[1]['zephyr_7b_beta_preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b18b47-76ec-429a-8b74-cfd071a5f52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Re-create the DataFrame and normalize data for radar chart\n",
    "data = {\n",
    "    'Metric': [\n",
    "        'BertScore Avg Precision', 'BertScore Avg Recall', 'BertScore Avg F1', \n",
    "        'Average Frugal Score', 'TER Score', 'BLEU Score', 'Rouge1', 'Rouge2', \n",
    "        'RougeL', 'RougeLsum', 'Exact Match Score', 'Average Manual Score'\n",
    "    ],\n",
    "    'Zephyr-7b-beta': [\n",
    "        0.925, 0.932, 0.929, 0.933, 1/72.787, 0.388, 0.591, 0.406, 0.537, 0.565, 0.235, 6.236\n",
    "    ],\n",
    "    'Llama-2-7b-chat-hf': [\n",
    "        0.890, 0.924, 0.907, 0.909, 1/174.858, 0.176, 0.420, 0.267, 0.352, 0.392, 0.112, 5.713\n",
    "    ],\n",
    "    'Gemma-1.1-7b-it': [\n",
    "        0.936, 0.939, 0.937, 0.942, 1/56.214, 0.477, 0.645, 0.483, 0.587, 0.604, 0.208, 9.050\n",
    "    ],\n",
    "    'Meta-Llama-3-8B-Instruct': [\n",
    "        0.914, 0.916, 0.915, 0.923, 1/77.212, 0.314, 0.519, 0.314, 0.443, 0.477, 0.143, 8.967\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Normalize data for radar chart\n",
    "def normalize(column):\n",
    "    max_val = df[column].max()\n",
    "    min_val = df[column].min()\n",
    "    df[column] = (df[column] - min_val) / (max_val - min_val)\n",
    "\n",
    "# Normalize all columns except the Metric column\n",
    "for column in df.columns[1:]:\n",
    "    normalize(column)\n",
    "\n",
    "# Radar chart\n",
    "labels = df['Metric'].tolist()\n",
    "num_vars = len(labels)\n",
    "\n",
    "# Compute angle for each axis\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "\n",
    "# The plot is made in a circular (not polygon) space\n",
    "angles += angles[:1]\n",
    "\n",
    "# Function to plot radar chart\n",
    "def plot_radar_chart_adjusted_fixed(data, title):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    # Draw one axe per variable and add labels\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.xticks(angles[:-1], labels, color='grey', size=8)\n",
    "\n",
    "    # Adjust ylabels to be more spaced and readable\n",
    "    plt.yticks([0.1, 0.3, 0.5, 0.7, 0.9], [\"0.1\", \"0.3\", \"0.5\", \"0.7\", \"0.9\"], color=\"grey\", size=7)\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    for model, values in data.items():\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, linewidth=1, linestyle='solid', label=model)\n",
    "        ax.fill(angles, values, alpha=0.25)\n",
    "\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.title(title, size=20, color='black', y=1.1)\n",
    "\n",
    "# Prepare data for radar chart\n",
    "data_for_chart = {\n",
    "    'Zephyr-7b-beta': df['Zephyr-7b-beta'].tolist(),\n",
    "    'Llama-2-7b-chat-hf': df['Llama-2-7b-chat-hf'].tolist(),\n",
    "    'Gemma-1.1-7b-it': df['Gemma-1.1-7b-it'].tolist(),\n",
    "    'Meta-Llama-3-8B-Instruct': df['Meta-Llama-3-8B-Instruct'].tolist()\n",
    "}\n",
    "\n",
    "# Plot radar chart with adjusted axes\n",
    "plot_radar_chart_adjusted_fixed(data_for_chart, 'Model Evaluation Comparison (TER Score Inverted)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b962506d-57e5-4b3e-8895-49ff745e7552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "gemma_ds = datasets.load_from_disk('E:\\\\files\\\\Stuttgart\\\\Thesis\\\\NLP\\\\saved_datasets\\\\final_dataset_gemma_09052024\\\\my_gemma_after_prediction_23052024')\n",
    "\n",
    "llama2_ds = datasets.load_from_disk('E:\\\\files\\\\Stuttgart\\\\Thesis\\\\NLP\\\\saved_datasets\\\\final_dataset_for_training_17042024\\\\my_llama_after_prediction_01052024')\n",
    "\n",
    "zephyr_ds = datasets.load_from_disk('E:\\\\files\\\\Stuttgart\\\\Thesis\\\\NLP\\\\saved_datasets\\\\final_dataset_for_training_17042024\\\\my_zepyra_after_prediction_17040204')\n",
    "\n",
    "\n",
    "llama3_ds = datasets.load_from_disk('E:\\\\files\\\\Stuttgart\\\\Thesis\\\\NLP\\\\saved_datasets\\\\final_dataset_llama3_07052024\\\\my_llama3_after_prediction_08052024')\n",
    "\n",
    "llama3_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1c857-202b-4451-94f2-050d0aca0ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch bert-score sacrebleu rouge-score nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6848f89-5ffa-478f-aa7c-fa884612599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bceeda-82dd-47fa-9ca1-19695258995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data from the original image and the requested changes\n",
    "data = {\n",
    "    'Metric': [\n",
    "        'BertScore Avg Precision', 'BertScore Avg Recall', 'BertScore Avg F1',\n",
    "        'Average Frugal Score', 'TER Score', 'BLEU Score', 'Rouge1',\n",
    "        'Rouge2', 'RougeL', 'RougeLsum', 'Exact Match Score', 'Average Manual Score'\n",
    "    ],\n",
    "    'Zephyr-7b-beta': [\n",
    "        0.925, 0.932, 0.929, 0.933, 72.787, 0.388, 0.591,\n",
    "        0.406, 0.537, 0.565, 0.235, -0.476\n",
    "    ],\n",
    "    'Llama-2-7b-chat-hf': [\n",
    "        0.890, 0.924, 0.907, 0.909, 174.858, 0.176, 0.420,\n",
    "        0.267, 0.352, 0.392, 0.112, -0.637\n",
    "    ],\n",
    "    'Gemma-1.1-7b-it': [\n",
    "        0.936, 0.939, 0.937, 0.942, 56.214, 0.477, 0.645,\n",
    "        0.483, 0.587, 0.604, 0.208, -0.327\n",
    "    ],\n",
    "    '*Meta-Llama-3-8B-Instruct': [\n",
    "        0.914, 0.916, 0.915, 0.923, 77.212, 0.314, 0.519,\n",
    "        0.314, 0.443, 0.477, 0.143, -0.190\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Set the style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a matplotlib figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create a table plot\n",
    "table = plt.table(cellText=df.values, colLabels=df.columns, cellLoc='center', loc='center')\n",
    "\n",
    "# Modify table properties\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)\n",
    "\n",
    "# Hide the axes\n",
    "plt.axis('off')\n",
    "\n",
    "# Save the table as an image\n",
    "plt.savefig('updated_table.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94759c0b-a8f5-4ea8-ba48-5fa13f246a09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
